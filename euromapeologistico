python
import numpy as np
import pandas as pd

# Leer el archivo CSV con los datos
df = pd.read_csv('jpt_data.csv', header=None)

# Convertir el dataframe en un array de NumPy
numeros = df.to_numpy()

# Obtener el número de filas y columnas del array
n_filas, n_columnas = numeros.shape

# Aplicar el mapeo logístico a cada número
r = 3.99 # Parámetro de crecimiento
x = numeros / 50 # Normalizar los números entre 0 y 1
for i in range(100): # Iterar 100 veces
    x = r * x * (1 - x) # Aplicar la ecuación logística

# Redondear los números al entero más cercano y limitarlos al rango de 1 a 50
x = np.round(x * 50)
x = np.clip(x, 1, 50)

# Guardar los números en un archivo CSV
df = pd.DataFrame(x)
df.to_csv('mapeo_logistico.csv', index=False, header=False)


*eurokeras_scikit.py:*

python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense

# Leer el archivo CSV con los datos
df = pd.read_csv('jpt_data.csv', header=None)

# Separar los datos en variables independientes (X) y dependiente (y)
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

# Estandarizar los datos usando StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X)

# Crear el modelo de Keras con una capa oculta de 10 neuronas y una capa de salida de 1 neurona
model = Sequential()
model.add(Dense(10, input_dim=X.shape[1], activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compilar el modelo usando el optimizador adam y la función de pérdida binary_crossentropy
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Entrenar el modelo con los datos
model.fit(X, y, epochs=100, batch_size=10)

# Evaluar el modelo con los mismos datos
scores = model.evaluate(X, y)
print('Precisión: %.2f' % (scores[1] * 100))
